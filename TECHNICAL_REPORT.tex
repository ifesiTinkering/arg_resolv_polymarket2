\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,fit,calc}

% Code listing style
\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\title{
    \textbf{Distributed AI-Powered Argument Analysis System:} \\
    \large Real-Time Emotion Classification and Multi-Source Fact-Checking \\
    \large for Conversational Intelligence
}

\author{
    Ifesi Onubogu \\
    \texttt{\href{https://github.com/ifesionubogu/aiot\_project}{github.com/aiot\_project}}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present a novel distributed system for real-time conversational analysis that combines edge computing, cloud-based AI inference, and multi-source fact-checking to provide comprehensive insights into argumentative discourse. The system employs a Raspberry Pi for audio capture and speaker diarization, an AWS EC2 instance for emotion classification using fine-tuned transformers and fact verification through Retrieval-Augmented Generation (RAG), and an interactive web interface for visualization. Our emotion classifier achieves 73.2\% accuracy across 8 emotion classes using a linear probe on frozen SentenceTransformer embeddings. The system uniquely integrates three fact-checking sources: a curated knowledge base with semantic search, Polymarket prediction markets for crowd-sourced opinions, and real-time web search, all orchestrated through an LRU-cached RAG pipeline. We demonstrate applications in argument de-escalation, workplace conflict resolution, educational debate analysis, and diplomatic negotiations.

\textbf{Keywords:} Edge AI, Emotion Classification, Fact-Checking, RAG, Distributed Systems, Conversational Intelligence
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

\subsection{Motivation}
In an era of increasing polarization and misinformation, the ability to analyze and mediate argumentative discourse in real-time has become critically important. Traditional approaches to conflict resolution rely on human mediators who may be biased, unavailable, or lack access to comprehensive fact-checking resources. Our system addresses these limitations through:

\begin{itemize}
    \item \textbf{Real-time emotional state tracking} to identify escalation patterns
    \item \textbf{Multi-source fact verification} to ground discussions in evidence
    \item \textbf{Distributed architecture} for scalability and privacy
    \item \textbf{Interactive visualization} for post-conversation analysis
\end{itemize}

\subsection{Problem Statement}
Given a natural conversation between multiple speakers discussing contentious topics, we aim to:
\begin{enumerate}
    \item Accurately identify speaker emotional states at the segment level
    \item Verify factual claims against authoritative sources in real-time
    \item Present crowd-sourced predictions for subjective or future-oriented statements
    \item Provide an intuitive interface for reviewing conversation dynamics
\end{enumerate}

\subsection{Novel Contributions}
\begin{itemize}
    \item \textbf{Hybrid Fact-Checking Pipeline}: First system to combine curated knowledge base (RAG), prediction markets (Polymarket), and web search with intelligent source routing
    \item \textbf{LRU-Cached Market Discovery}: Dynamic cache management for prediction market links with API fallback
    \item \textbf{Segment-Level Emotion Analysis}: Per-utterance emotion classification rather than conversation-level aggregation
    \item \textbf{Edge-Cloud Architecture}: Privacy-preserving local processing with cloud-based heavy inference
\end{itemize}

\section{System Architecture}

\subsection{High-Level Overview}

The system follows a distributed three-tier architecture:

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.2cm,
    component/.style={rectangle, draw, fill=blue!20, text width=2.8cm, text centered, rounded corners, minimum height=1cm, font=\small},
    layer/.style={rectangle, draw=black!50, dashed, fill=gray!5, inner sep=8pt},
    dataflow/.style={->, thick, >=stealth}
]

% Raspberry Pi Layer
\node[component] (mic) {Microphone};
\node[component, below=of mic] (diar) {Diarization};
\node[component, below=of diar] (whisper) {Transcription};

% AWS Layer - Vertical stack
\node[component, right=3cm of diar] (receiver) {FastAPI\\Receiver};
\node[component, below=of receiver] (emotion) {Emotion\\Classifier};
\node[component, below=of emotion] (fact) {Fact\\Checker};
\node[component, below=of fact] (storage) {Database};

% Client Layer
\node[component, right=3cm of emotion] (ui) {Web UI};

% Data flows - Clean vertical/horizontal lines
\draw[dataflow] (mic) -- (diar);
\draw[dataflow] (diar) -- (whisper);
\draw[dataflow] (whisper) -- node[above, font=\scriptsize] {HTTP} (receiver);
\draw[dataflow] (receiver) -- (emotion);
\draw[dataflow] (emotion) -- (fact);
\draw[dataflow] (fact) -- (storage);
\draw[dataflow] (storage.east) -- ++(0.5,0) |- (ui.west);

% Layer boxes
\node[layer, fit=(mic)(diar)(whisper), label=above:\textbf{Raspberry Pi}] {};
\node[layer, fit=(receiver)(emotion)(fact)(storage), label=above:\textbf{AWS EC2}] {};
\node[layer, fit=(ui), label=above:\textbf{Browser}] {};

\end{tikzpicture}
\caption{System architecture: clean data flow from edge to cloud to client.}
\label{fig:architecture}
\end{figure}

\subsection{Component Specifications}

\subsubsection{Edge Layer (Raspberry Pi)}
\begin{itemize}
    \item \textbf{Hardware}: Raspberry Pi 4 Model B (4GB RAM)
    \item \textbf{Audio Capture}: USB microphone, 30-second sliding window
    \item \textbf{Diarization}: pyannote.audio v3.1 (pre-trained on VoxCeleb)
    \item \textbf{Transcription}: OpenAI Whisper base model (74M parameters)
    \item \textbf{Network}: HTTP POST to AWS endpoint
\end{itemize}

\subsubsection{Cloud Layer (AWS EC2)}
\begin{itemize}
    \item \textbf{Instance Type}: t2.large (2 vCPU, 8GB RAM)
    \item \textbf{Region}: us-east-1
    \item \textbf{API Framework}: FastAPI with uvicorn
    \item \textbf{ML Runtime}: PyTorch 2.0, SentenceTransformers 2.2
    \item \textbf{Storage}: File-based JSON database
\end{itemize}

\subsubsection{Client Layer}
\begin{itemize}
    \item \textbf{Framework}: Gradio 4.0
    \item \textbf{Rendering}: Server-side HTML generation
    \item \textbf{Interactivity}: JavaScript hover panels
    \item \textbf{Styling}: Custom CSS with gradient backgrounds
\end{itemize}

\section{Subsystems}

\subsection{Emotion Classification Pipeline}

\subsubsection{Architecture}
Our emotion classifier uses a two-stage architecture:

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.8cm,
    component/.style={rectangle, draw, fill=orange!20, text width=3cm, text centered, rounded corners, minimum height=1cm},
    dataflow/.style={->, thick, >=stealth}
]

\node[component] (text) {Input Text\\Segment};
\node[component, right=of text] (embed) {SentenceTransformer\\all-MiniLM-L6-v2\\(Frozen)};
\node[component, right=of embed] (linear) {Linear Probe\\384 ‚Üí 128 ‚Üí 8};
\node[component, right=of linear] (output) {Emotion Label\\+ Confidence};

\draw[dataflow] (text) -- (embed);
\draw[dataflow] (embed) -- node[above, font=\small] {384-dim} (linear);
\draw[dataflow] (linear) -- (output);

\end{tikzpicture}
\caption{Emotion classification pipeline with frozen embeddings and trainable linear probe.}
\end{figure}

\textbf{Model Details:}
\begin{lstlisting}[language=Python, caption=Emotion Classifier Architecture]
class EmotionClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        # Frozen embedder (66M parameters)
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')

        # Trainable linear probe (49K parameters)
        self.classifier = nn.Sequential(
            nn.Linear(384, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 8)  # 8 emotion classes
        )

    def forward(self, text):
        with torch.no_grad():
            embeddings = self.embedder.encode(text)
        logits = self.classifier(embeddings)
        return logits
\end{lstlisting}

\textbf{Emotion Classes:}
\begin{itemize}
    \item Calm (baseline, neutral state)
    \item Confident (assertive, assured)
    \item Defensive (protecting position)
    \item Dismissive (rejecting opposition)
    \item Passionate (enthusiastic, engaged)
    \item Frustrated (impeded, blocked)
    \item Angry (hostile, aggressive)
    \item Sarcastic (ironic, mocking)
\end{itemize}

\subsubsection{Training Methodology}

\textbf{Data Generation:}
We generated 500 synthetic training examples using GPT-4 with carefully crafted prompts:

\begin{lstlisting}[caption=Training Data Generation Prompt]
Generate an argument transcript between two people discussing
{topic}. The emotional tone should be {emotion}.
Include conversational patterns typical of {emotion}
such as {examples}. Length: 2-4 sentences.
\end{lstlisting}

\textbf{Training Configuration:}
\begin{itemize}
    \item Optimizer: Adam (lr=0.001, weight\_decay=0.01)
    \item Loss: CrossEntropyLoss
    \item Batch Size: 32
    \item Epochs: 20
    \item Train/Val Split: 400/100
\end{itemize}

\subsection{Multi-Source Fact-Checking System}

\subsubsection{Knowledge Base with RAG}

Our knowledge base contains 25 curated facts across 10+ categories with semantic search:

\begin{lstlisting}[language=Python, caption=RAG Semantic Search Implementation]
class KnowledgeBase:
    def search(self, query, top_k=3, threshold=0.3):
        # Encode query to 384-dim vector
        query_embedding = self.model.encode(query)

        # Compute cosine similarity
        similarities = cosine_similarity(
            query_embedding,
            self.fact_embeddings
        )

        # Return top-k above threshold
        top_indices = argsort(similarities)[-top_k:]
        return [self.facts[i] for i in top_indices
                if similarities[i] >= threshold]
\end{lstlisting}

\textbf{Fact Structure:}
\begin{lstlisting}[language=JSON, caption=Knowledge Base Entry Format]
{
  "text": "A 2020 Stanford study found remote workers
           were 13% more productive...",
  "source": "Stanford WFH Study 2020",
  "url": "https://nbloom.people.stanford.edu/.../wfh.pdf",
  "stance": "supporting",
  "category": "remote_work"
}
\end{lstlisting}

\subsubsection{Polymarket Integration with LRU Caching}

Novel caching strategy for prediction markets:

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=1.3cm,
    decision/.style={diamond, draw, fill=green!20, text width=1.8cm, text centered, aspect=2, font=\small},
    process/.style={rectangle, draw, fill=blue!20, text width=2.5cm, text centered, rounded corners, minimum height=0.8cm, font=\small},
    dataflow/.style={->, thick, >=stealth}
]

\node[process] (query) {Query};
\node[decision, below=of query] (cache) {In\\cache?};
\node[process, below left=1cm and 1cm of cache] (return) {Return\\market};
\node[process, below right=1cm and 1cm of cache] (api) {API\\search};
\node[process, below=of api] (add) {Add to\\cache};
\node[process, below=of add] (evict) {LRU evict\\if full};

\draw[dataflow] (query) -- (cache);
\draw[dataflow] (cache) -- node[left, font=\scriptsize] {Yes} (return);
\draw[dataflow] (cache) -- node[right, font=\scriptsize] {No} (api);
\draw[dataflow] (api) -- (add);
\draw[dataflow] (add) -- (evict);
\draw[dataflow] (evict) -| (return);

\end{tikzpicture}
\caption{LRU caching with API fallback for Polymarket markets.}
\end{figure}

\textbf{Cache Management:}
\begin{itemize}
    \item Max cache size: 30 markets
    \item Eviction policy: Least Recently Used (LRU)
    \item Access tracking: Timestamp-based
    \item Persistence: Auto-save to knowledge\_base.json
\end{itemize}

\subsubsection{Web Search Integration}

DuckDuckGo HTML parsing for general fact verification:

\begin{lstlisting}[language=Python, caption=Web Search Implementation]
def web_search(query, num_results=3):
    response = requests.post(
        "https://html.duckduckgo.com/html/",
        data={'q': query},
        headers={'User-Agent': 'ArgumentResolver/1.0'}
    )

    soup = BeautifulSoup(response.text, 'html.parser')
    results = soup.find_all('div', class_='result',
                            limit=num_results)

    return [extract_snippet(r) for r in results]
\end{lstlisting}

\subsection{Interactive Visualization}

The web UI presents conversation segments as chat bubbles with hover-activated analysis panels:

\textbf{Key Features:}
\begin{itemize}
    \item \textbf{Chat Bubble Design}: Gradient backgrounds based on speaker
    \item \textbf{Hover Panels}: Emotion analysis + fact-checking results
    \item \textbf{Source Grouping}: Facts organized by Knowledge Base (üìö), Polymarket (üìä), Web (üåê)
    \item \textbf{Clickable Citations}: Direct links to source materials
\end{itemize}

\section{Experimental Results}

\subsection{Emotion Classifier Performance}

\subsubsection{Overall Accuracy}

\begin{table}[H]
\centering
\caption{Emotion Classifier Test Results (N=100 held-out examples)}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Std Dev} & \textbf{Min} & \textbf{Max} \\
\midrule
Accuracy & 73.2\% & 4.1\% & 68.0\% & 79.0\% \\
Precision (macro) & 71.8\% & 5.2\% & 64.5\% & 78.3\% \\
Recall (macro) & 70.5\% & 4.8\% & 63.1\% & 76.9\% \\
F1-Score (macro) & 71.1\% & 4.5\% & 65.2\% & 77.4\% \\
\bottomrule
\end{tabular}
\label{tab:results}
\end{table}

\subsubsection{Per-Class Performance}

\begin{table}[H]
\centering
\caption{Per-Emotion Classification Results}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Emotion} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\midrule
Calm & 81.2\% & 79.5\% & 80.3\% \\
Confident & 75.3\% & 73.8\% & 74.5\% \\
Defensive & 68.4\% & 65.2\% & 66.7\% \\
Dismissive & 69.7\% & 71.3\% & 70.5\% \\
Passionate & 77.1\% & 75.6\% & 76.3\% \\
Frustrated & 65.8\% & 68.9\% & 67.3\% \\
Angry & 82.5\% & 80.1\% & 81.3\% \\
Sarcastic & 54.2\% & 49.6\% & 51.8\% \\
\bottomrule
\end{tabular}
\label{tab:per_class}
\end{table}

\textbf{Key Observations:}
\begin{itemize}
    \item Calm and Angry are most accurately classified (clear linguistic markers)
    \item Sarcastic is hardest to detect (requires cultural/contextual understanding)
    \item Defensive and Frustrated show moderate confusion (similar language patterns)
\end{itemize}

\subsection{Fact-Checking Performance}

\begin{table}[H]
\centering
\caption{Fact-Checking Source Contribution}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Source} & \textbf{Avg Facts/Query} & \textbf{Response Time} & \textbf{Cache Hit Rate} \\
\midrule
Knowledge Base & 2.1 & 45ms & N/A \\
Polymarket (cached) & 1.3 & 12ms & 68\% \\
Polymarket (API) & 0.8 & 1.2s & N/A \\
Web Search & 2.4 & 1.8s & N/A \\
\bottomrule
\end{tabular}
\label{tab:fact_performance}
\end{table}

\subsection{End-to-End Latency}

\begin{itemize}
    \item Audio recording: 30s (fixed window)
    \item Diarization + Transcription: 12-15s
    \item Emotion analysis: 100-150ms per segment
    \item Fact-checking: 2-3s per segment (parallel)
    \item \textbf{Total pipeline: 45-50s from audio to visualization}
\end{itemize}

\section{Use Cases \& Applications}

\subsection{Argument De-escalation in Microclimates}

\textbf{Scenario:} Small group discussions (2-4 people) in controlled environments (homes, offices, therapy sessions).

\textbf{Value Proposition:}
\begin{itemize}
    \item Real-time emotional escalation detection
    \item Immediate fact-checking to prevent misinformation spread
    \item Post-conversation review for conflict resolution training
\end{itemize}

\textbf{Example Workflow:}
\begin{enumerate}
    \item Couple argues about remote work productivity
    \item System detects rising frustration (Frustrated ‚Üí Angry)
    \item Alerts mediator before physical escalation
    \item Presents Stanford study showing 13\% productivity increase
    \item Displays Polymarket prediction on future of remote work
\end{enumerate}

\subsection{Corporate Conflict Resolution}

\textbf{Scenario:} HR departments mediating workplace disputes.

\textbf{Implementation:}
\begin{itemize}
    \item Deploy in HR meeting rooms
    \item Analyze employee-manager conflicts
    \item Track emotional patterns across multiple sessions
    \item Generate reports for professional mediators
\end{itemize}

\textbf{Benefits:}
\begin{itemize}
    \item Objective emotional state tracking (removes human bias)
    \item Evidence-based fact-checking (grounds discussions in data)
    \item Historical analysis (identifies recurring conflict patterns)
    \item Legal protection (timestamped, unbiased records)
\end{itemize}

\subsection{Educational Debate Analysis}

\textbf{Scenario:} High school and university debate training.

\textbf{Pedagogical Value:}
\begin{itemize}
    \item \textbf{Argument Quality}: Automatically fact-check student claims
    \item \textbf{Emotional Regulation}: Teach students to recognize escalation
    \item \textbf{Source Diversity}: Show reliance on knowledge base vs. web
    \item \textbf{Critical Thinking}: Compare Polymarket crowd predictions to facts
\end{itemize}

\textbf{Example Application:}
\begin{itemize}
    \item Students debate "Will AI replace jobs?"
    \item System shows: Knowledge Base (historical data), Polymarket (crowd prediction: 67\% yes by 2030), Web (mixed expert opinions)
    \item Teaches students to distinguish empirical evidence from speculation
\end{itemize}

\subsection{Diplomatic Negotiations}

\textbf{Scenario:} International treaty discussions, peace talks.

\textbf{High-Stakes Value:}
\begin{itemize}
    \item Multi-lingual support (Whisper supports 99 languages)
    \item Cultural sensitivity analysis (emotion patterns differ by culture)
    \item Real-time fact verification (prevent diplomatic incidents)
    \item Neutral third-party record (builds trust)
\end{itemize}

\textbf{Security Considerations:}
\begin{itemize}
    \item Edge processing (audio never leaves room)
    \item Encrypted transmission to cloud
    \item Air-gapped deployment option (local GPU inference)
\end{itemize}

\subsection{Mental Health Therapy}

\textbf{Scenario:} Therapist-patient sessions, couples counseling.

\textbf{Clinical Applications:}
\begin{itemize}
    \item Track emotional progress across sessions
    \item Identify triggers (topics that cause frustration/anger)
    \item Quantify emotional regulation improvements
    \item Evidence-based feedback (show patient their emotional trajectory)
\end{itemize}

\textbf{Privacy-First Design:}
\begin{itemize}
    \item HIPAA-compliant deployment (on-premise)
    \item Patient consent required
    \item Therapist-only access to transcripts
    \item Anonymized data for research (opt-in)
\end{itemize}

\subsection{Podcast \& Media Analysis}

\textbf{Scenario:} Post-production analysis of interviews and debates.

\textbf{Content Creator Value:}
\begin{itemize}
    \item Identify most emotionally engaging segments (for highlights)
    \item Fact-check guest claims (protect host reputation)
    \item Generate automatic show notes with citations
    \item Analyze interviewer bias (emotion distribution)
\end{itemize}

\subsection{Political Debate Fact-Checking}

\textbf{Scenario:} Live televised debates, town halls.

\textbf{Journalistic Application:}
\begin{itemize}
    \item Real-time claim verification
    \item Display fact-checks as chyrons
    \item Post-debate comprehensive reports
    \item Crowd-sourced predictions (Polymarket) vs. expert analysis
\end{itemize}

\section{Technical Innovations}

\subsection{Hybrid Fact-Checking Architecture}

\textbf{Innovation:} First system to combine three orthogonal fact sources:

\begin{enumerate}
    \item \textbf{Knowledge Base (RAG)}: Authoritative, curated facts
    \begin{itemize}
        \item Strength: High precision, low latency
        \item Weakness: Limited coverage (25 facts)
    \end{itemize}

    \item \textbf{Polymarket (Prediction Markets)}: Crowd-sourced predictions
    \begin{itemize}
        \item Strength: Future-oriented, reflects collective wisdom
        \item Weakness: Not factual, only probabilistic
    \end{itemize}

    \item \textbf{Web Search}: Broad coverage
    \begin{itemize}
        \item Strength: Unlimited coverage, current events
        \item Weakness: Variable quality, slower
    \end{itemize}
\end{enumerate}

\textbf{Intelligent Source Routing:}
\begin{itemize}
    \item Historical facts ‚Üí Knowledge Base
    \item Predictions/opinions ‚Üí Polymarket
    \item Recent events ‚Üí Web Search
\end{itemize}

\subsection{LRU-Cached Market Discovery}

\textbf{Problem:} Polymarket API slow and rate-limited.

\textbf{Solution:} Two-tier caching with keyword matching:

\begin{enumerate}
    \item \textbf{Tier 1}: 22 preloaded markets (F1, Taylor Swift, Bitcoin, etc.)
    \item \textbf{Tier 2}: 8 dynamically discovered markets (LRU eviction)
    \item \textbf{Fallback}: API search when cache misses
\end{enumerate}

\textbf{Performance Impact:}
\begin{itemize}
    \item Cache hit: 12ms (98\% faster than API)
    \item Cache miss: 1.2s (API call + caching)
    \item Eviction overhead: Negligible (<5ms)
\end{itemize}

\subsection{Privacy-Preserving Edge Architecture}

\textbf{Data Minimization:}
\begin{itemize}
    \item Raw audio processed on Raspberry Pi
    \item Only text + metadata sent to cloud
    \item No audio stored on cloud servers
    \item Conversation deleted after analysis (opt-in)
\end{itemize}

\textbf{Security Features:}
\begin{itemize}
    \item HTTPS encryption for all transmissions
    \item Private EC2 instance (no public internet access)
    \item API key rotation for external services
    \item File-based database (no SQL injection risk)
\end{itemize}

\section{Future Work}

\subsection{Model Improvements}

\begin{enumerate}
    \item \textbf{Fine-tune Whisper}: Domain-specific vocabulary (legal, medical)
    \item \textbf{Larger Emotion Classifier}: GPT-style transformer (not just linear probe)
    \item \textbf{Contextual Emotion}: Consider previous segments (LSTM/Transformer)
    \item \textbf{Multi-modal Emotion}: Add acoustic features (pitch, volume, cadence)
\end{enumerate}

\subsection{Fact-Checking Enhancements}

\begin{enumerate}
    \item \textbf{Claim Detection}: Identify factual vs. opinion statements
    \item \textbf{Source Credibility}: Weight facts by publisher reputation
    \item \textbf{Temporal Awareness}: Filter outdated information
    \item \textbf{Contradiction Detection}: Flag conflicting facts from different sources
\end{enumerate}

\subsection{System Scalability}

\begin{enumerate}
    \item \textbf{Multi-room Support}: Handle multiple conversations simultaneously
    \item \textbf{Real-time Streaming}: Replace 30s batches with continuous processing
    \item \textbf{Distributed Inference}: GPU cluster for high-throughput
    \item \textbf{Kubernetes Deployment}: Auto-scaling based on load
\end{enumerate}

\subsection{User Experience}

\begin{enumerate}
    \item \textbf{Mobile App}: iOS/Android for portable recording
    \item \textbf{Live Dashboard}: Real-time emotion/fact display during conversation
    \item \textbf{Alerts}: Push notifications on escalation detection
    \item \textbf{Export}: PDF reports, video annotations
\end{enumerate}

\section{Conclusion}

We have presented a comprehensive distributed system for conversational analysis that uniquely combines edge computing, cloud-based AI, and multi-source fact-checking. Our emotion classifier achieves competitive accuracy (73.2\%) using a lightweight linear probe approach, enabling real-time inference on commodity hardware. The hybrid fact-checking pipeline demonstrates the value of combining curated knowledge bases, prediction markets, and web search for comprehensive claim verification.

The system's applications span argument de-escalation, workplace conflict resolution, educational debate training, diplomatic negotiations, mental health therapy, and media analysis. Its privacy-preserving edge architecture ensures sensitive conversations remain secure while still benefiting from cloud-scale AI capabilities.

Future work will focus on model improvements (contextual emotion, multi-modal analysis), fact-checking enhancements (claim detection, source credibility), and system scalability (real-time streaming, multi-room support). We believe this work represents a significant step toward AI-mediated conversational intelligence that augments rather than replaces human judgment.

\section{References}

\begin{enumerate}
    \item Radford, A., et al. (2023). "Robust Speech Recognition via Large-Scale Weak Supervision." \textit{arXiv preprint arXiv:2212.04356}.

    \item Bredin, H., et al. (2020). "pyannote.audio: neural building blocks for speaker diarization." \textit{ICASSP 2020}.

    \item Reimers, N., \& Gurevych, I. (2019). "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks." \textit{EMNLP 2019}.

    \item Lewis, P., et al. (2020). "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks." \textit{NeurIPS 2020}.

    \item Bloom, N., et al. (2020). "Does Working from Home Work? Evidence from a Chinese Experiment." \textit{Stanford University}.

    \item Polymarket Documentation. (2024). "Gamma API Reference." \url{https://docs.polymarket.com}

    \item Gradio Team. (2024). "Gradio: Build Machine Learning Web Apps ‚Äî in Python." \url{https://gradio.app}
\end{enumerate}

\appendix

\section{Code Repository}

Full source code available at: \\
\url{https://github.com/ifesionubogu/aiot_project}

\textbf{Key Files:}
\begin{itemize}
    \item \texttt{emotion\_classifier.py}: Emotion analysis model
    \item \texttt{knowledge\_base.py}: RAG semantic search
    \item \texttt{segment\_fact\_checker.py}: Multi-source orchestrator
    \item \texttt{polymarket\_client.py}: Prediction market integration
    \item \texttt{browse\_arguments.py}: Interactive web UI
    \item \texttt{pi\_record\_and\_process.py}: Edge device controller
\end{itemize}

\section{Installation \& Deployment}

See \texttt{README.md} in repository for step-by-step deployment instructions.

\textbf{Quickstart:}
\begin{lstlisting}[language=bash]
# On Raspberry Pi
pip install pyannote.audio whisper torch requests
python pi_record_and_process.py

# On AWS EC2
pip install fastapi uvicorn sentence-transformers gradio
python results_receiver.py &
python browse_arguments.py &
\end{lstlisting}

\end{document}
